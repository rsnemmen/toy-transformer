{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffac3e3c-be33-43b9-8ee8-485db83f5c04",
   "metadata": {},
   "source": [
    "# Numpy implementation of scaled-dot product attention\n",
    "\n",
    "- Uses only numpy.\n",
    "- Considers weight matrices $W_{q,k,v}$ and query, key, values $q, k, v$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7ba68d-a0c1-4004-a6dd-10e36b8e80e3",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5787fa63-6b74-4c41-b009-547d6ae66bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee342c0-f157-4748-bb2c-bbc99fab40ee",
   "metadata": {},
   "source": [
    "### Dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ea7bda9-7270-499b-9d0f-283e9b22467f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many tokens?\n",
    "ntokens=4\n",
    "\n",
    "# dimensionality of token embedding space\n",
    "dim=3\n",
    "\n",
    "# dimensionality: q, v, k embeddings\n",
    "dk=2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8068d8-7a5e-4be7-947e-1fc07e26c242",
   "metadata": {},
   "source": [
    "### Input token\n",
    "\n",
    "(I am skipping the step of tokenizing a string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af62a950-c2f3-4d93-ab18-ab4a3a5ad383",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=np.random.randint(0, 100, size=ntokens, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47566525-22f3-453b-83f2-af36b2525e99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12,  4, 27, 70])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee919c94-71cf-4456-afac-7c28745e92ef",
   "metadata": {},
   "source": [
    "## Generates mock embedding vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891f6f81-815d-482f-942e-1e2d90a6fc5f",
   "metadata": {},
   "source": [
    "For the sake of going straight into implementing attention, I will skip the preprocessing steps of converting a token into an embedding vector. I will simply initialize vectors of dimension 3 with random numbers as components, according to the number of tokens provided.\n",
    "\n",
    "Here is a **random array representing the embedding vectors** $X$, where each line corresponds to a different vector $x^{(i)}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f35251f7-9c8f-4587-9b37-f748034d378c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a matrix of size n x m with values between 0 and 1\n",
    "input = np.random.rand(ntokens, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7495b20f-7909-4aff-af73-cfe74a6b3a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.64035478, 0.34173638, 0.15410133],\n",
       "       [0.80115281, 0.54747531, 0.7199265 ],\n",
       "       [0.10293235, 0.03616533, 0.51787903],\n",
       "       [0.7233325 , 0.06916225, 0.85134328]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51e223f0-c87a-495d-b64e-1e0fe466e96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141edfad-8b22-408c-ba8d-99af5ef626b3",
   "metadata": {},
   "source": [
    "## Initializes weight matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85363560-ff22-4ab9-a38b-5885575ff7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_query=np.random.rand(dim, dk)\n",
    "W_key=np.random.rand(dim, dk)\n",
    "W_value=np.random.rand(dim, dk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bae3a4f-92ff-4241-94d2-a9fe14f7cb41",
   "metadata": {},
   "source": [
    "## Queries, keys and values\n",
    "\n",
    "$$ Q = X W_q$$\n",
    "$$ K = X W_k$$\n",
    "$$ V = X W_v$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a65ac6d-e4da-4cbc-a23b-70f9479e2bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = X @ W_query\n",
    "K = X @ W_key\n",
    "V = X @ W_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59736f7d-d055-48d7-9533-859f8b91590a",
   "metadata": {},
   "source": [
    "## Attention\n",
    "\n",
    "The attention is given by $${\\rm Attention}(Q,K,V) = {\\rm softmax}\\left( \\frac{QK^T}{\\sqrt{d_k}} \\right) V$$\n",
    "where $QK^T$ are the attention scores, softmax gives the attention weights and Attention is the context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed66c2f-db7f-4dc9-ba9f-14b414812f84",
   "metadata": {},
   "source": [
    "### Attention scores\n",
    "\n",
    "Given by $Q K^T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79f96557-ca84-470b-92e4-b5a4cbd43e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_scores = Q @ K.T "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8136be41-d8cb-45bf-808b-316c14dd7ee1",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "\n",
    "This softmax implementation is dumb and slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d56c58a3-5c5e-430b-ba69-bb6a2a9856cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    result=np.empty_like(X)\n",
    "        \n",
    "    for i,row in enumerate(X):\n",
    "        exps=np.exp(row)\n",
    "        result[i,:]=exps/exps.sum()\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168aed19-a242-443d-b2ed-315a854d3902",
   "metadata": {},
   "source": [
    "### Attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1126a391-aec6-42a4-b1a9-3ec86dcfd8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weights = softmax(attn_scores / K.shape[-1]**0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d698e975-2e65-431a-acfe-194b621c25ea",
   "metadata": {},
   "source": [
    "### Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "657ec911-63eb-4637-b150-de5ff6ca79a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = attn_weights @ V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c1d06a8-14b4-42d3-bcff-493c7e53aa65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.95090118, 0.45092381],\n",
       "       [1.00706272, 0.48109718],\n",
       "       [0.97225873, 0.46184897],\n",
       "       [1.01025764, 0.48263529]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33ad896-8f58-4c3e-9b96-e2c29a454057",
   "metadata": {},
   "source": [
    "# Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "42036600-7c13-4de9-b8bd-9b3564d1a5b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.24152842, 0.29440261, 0.24147565, 0.22259332],\n",
       "       [0.21538969, 0.37558925, 0.22729347, 0.18172759],\n",
       "       [0.21413151, 0.36706921, 0.23176097, 0.18703831],\n",
       "       [0.22252215, 0.33964576, 0.23726476, 0.20056732]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e569a93-5bf6-4856-9a89-7be3b66c2b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax2(x, axis=1):\n",
    "    exp_x = np.exp(x)  # Exponentiate the shifted input\n",
    "    sum_exp_x = np.sum(exp_x, axis=axis, keepdims=True)  # Sum along the specified axis\n",
    "    softmax_output = exp_x / sum_exp_x  # Normalize by the sum\n",
    "    return softmax_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9447902e-9339-4f99-9f64-59fadf198da0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.24814573, 0.28976314, 0.20758454, 0.25450659],\n",
       "       [0.23132908, 0.37280341, 0.13723455, 0.25863297],\n",
       "       [0.24106925, 0.31857965, 0.17960542, 0.26074569],\n",
       "       [0.22875064, 0.37683716, 0.13390889, 0.2605033 ]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2b8e0c07-9053-4723-aceb-5faf55ff752f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.24814573, 0.28976314, 0.20758454, 0.25450659],\n",
       "       [0.23132908, 0.37280341, 0.13723455, 0.25863297],\n",
       "       [0.24106925, 0.31857965, 0.17960542, 0.26074569],\n",
       "       [0.22875064, 0.37683716, 0.13390889, 0.2605033 ]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax2(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ac19a7-d132-4902-99e7-a1bb0bc66b61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
