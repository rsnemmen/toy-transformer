{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40bfffaf-a074-42fe-bb0b-3a2c00322f8f",
   "metadata": {},
   "source": [
    "# Mechanistic interpretability\n",
    "\n",
    "After reading the [Anthropic paper Elhage et al. (2021)](https://transformer-circuits.pub/2021/framework/index.html), I want to get a practical understanding of the basic concepts by implementing:\n",
    "\n",
    "- QK and OV matrices\n",
    "- skip-trigram\n",
    "\n",
    "## QK and OV matrices\n",
    "\n",
    "### Theory\n",
    "\n",
    "Using tensor calculus, the transformer can be represented as \n",
    "$$ T=\\mathrm{Id} \\otimes W_U W_E+\\sum_{h \\in H} A^h \\otimes\\left(W_U W_{O V}^h W_E\\right)$$\n",
    "where ${\\rm Id}$ is the identity matrix, $A^h$ are the attention for different attention heads given by \n",
    "$$A^h=\\operatorname{softmax}\\left(t^T \\cdot W_E^T W_{Q K}^h W_E \\cdot t\\right),$$ and $t$ are the input tokens. \n",
    "Here, $\\otimes$ is the outer product (or tensor product) and I am still not entirely sure what the $\\cdot$ represents. Presumably, matrix multiplication?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2482cb33-8312-4306-8f59-d95bffffe5ac",
   "metadata": {},
   "source": [
    "Now we introduce the \"query-key circuit\" QK and the \"output-value circuit\" OV, given by\n",
    "$$OV \\equiv W_U W_{O V}^h W_E$$\n",
    "and\n",
    "$$QK \\equiv W_E^T W_{Q K}^h W_E.$$\n",
    "\n",
    "The transformer can be then written as \n",
    "$$A^h=\\operatorname{softmax}\\left(t^T \\cdot QK \\cdot t\\right),$$\n",
    "$$ T=\\mathrm{Id} \\otimes W_U W_E+\\sum_{h \\in H} A^h \\otimes OV.$$\n",
    "The first term above encapsulates bigrams. The second, skip-trigrams."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3baf28-94de-45b6-88d9-cabd3e375d56",
   "metadata": {},
   "source": [
    "### Numpy implementation\n",
    "\n",
    "Notice that while I have handled the weight matrices Q, K and V, this actually involve a number of other weight matrices I am not familiar with. Without knowing more about these matrices, this is how I would implement QK and OV with numpy:\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "OV=WU @ WOV @ WE\n",
    "QK=WE.T @ WQK @ WE\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efbd51c-8cae-46b9-976f-e2a8ab09665d",
   "metadata": {},
   "source": [
    "Assuming `softmax` is already present, the transformer would then be implemented using the paper's formulation as\n",
    "```python\n",
    "# identity matrix with a given shape\n",
    "Id=np.eye(shape_I)\n",
    "\n",
    "# First transformer term\n",
    "T1=np.outer(Id,WU @ WE) \n",
    "\n",
    "# Second transformer term (totally inefficient)\n",
    "T2=0\n",
    "for QK in QK_list:\n",
    "    # Attention heads\n",
    "    Ah=softmax(t.T @ QK @ t)\n",
    "\n",
    "    # Second transformer term\n",
    "    T2+=np.outer(Ah,OV)\n",
    "\n",
    "T=T1+T2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3f4912-cba9-4d79-b800-cd6ef7242528",
   "metadata": {},
   "source": [
    "## Skip-trigram\n",
    "\n",
    "Here I implement an algorithm that prints all skip-trigram for a given sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594db565-9297-408a-ba0f-4f4bdb65ea3d",
   "metadata": {},
   "source": [
    "Input sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29464076-5a5c-424a-a72d-67628c6446c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq=range(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4419f50-6dfb-4cfd-ad7c-3e4cbc1160a4",
   "metadata": {},
   "source": [
    "### Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab260640-fe61-4759-ae61-29b12250b5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigram(seq):\n",
    "    for i in seq:\n",
    "        if i<len(seq)-2: \n",
    "            print(i,i+1,i+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40ce844e-b043-49f8-858e-492930d3b7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2\n",
      "1 2 3\n",
      "2 3 4\n",
      "3 4 5\n",
      "4 5 6\n",
      "5 6 7\n"
     ]
    }
   ],
   "source": [
    "trigram(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fef0d92-bfd4-472d-addd-5f4d8aae3f92",
   "metadata": {},
   "source": [
    "### Skip-trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61abc68e-3970-43b2-9c89-71e20716ce87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def skip_trigram(seq,step=1):\n",
    "    for i in seq:\n",
    "        if i<len(seq)-2-step: \n",
    "            print(i,i+1+step,i+2+step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4755589-73e2-4ba9-ad01-c703acf90d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2 3\n",
      "1 3 4\n",
      "2 4 5\n",
      "3 5 6\n",
      "4 6 7\n"
     ]
    }
   ],
   "source": [
    "skip_trigram(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8621e19d-134b-4a1c-9b40-d91b540b80d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
