Implementation of basic self-attention computations, in the form of Jupyter notebooks.

ðŸš§ Work in progress

```txt
.
â”œâ”€â”€ ðŸ“„ attention: simple attention w/o q,k,v in numpy
â”œâ”€â”€ ðŸ“„ scaled-dot-p: scaled-dot product attention using q,k,v; numpy only. For studying 
â”‚      purposes
â”œâ”€â”€ ðŸ“„ scaled-dot-p_comparison: scaled-dot product attention using q,k,v; 
â”‚      compares numpy vs pytorch implementation
â””â”€â”€ ðŸ“„ QK-OV: steps for computing the QK, OV matrices from the Elhage+2021 paper 
       (Anthropic). Also, bigrams and skip-trigrams
```