Implementation of basic self-attention computations, in the form of Jupyter notebooks.

ðŸš§ Work in progress

```txt
.
â”œâ”€â”€ ðŸ“„ attention: simple attention w/o q,k,v in numpy
â”œâ”€â”€ ðŸ“„ scaled-dot-p: scaled-dot product attention using q,k,v; numpy vs pytorch 
|       implementation
â”œâ”€â”€ ðŸ“„ scaled-dot-pâ€“clean: scaled-dot product attention using q,k,v; numpy only. For studying 
|      purposes
â””â”€â”€ ðŸ“„ QK-OV: steps for computing the QK, OV matrices from the Anthropic paper. Also, bigrams
       and skip-trigrams
```