Implementation of self-attention computations in preparation for the Anthropic research scientist interview on Nov 2024 (which I failed).

```txt
.
â”œâ”€â”€ ðŸ“„ attention: simple attention w/o q,k,v in numpy
â”œâ”€â”€ ðŸ“„ scaled-dot-p: scaled-dot product attention using q,k,v; numpy vs pytorch 
|       implementation
â”œâ”€â”€ ðŸ“„ scaled-dot-pâ€“clean: scaled-dot product attention using q,k,v; numpy only. For studying 
|      purposes
â””â”€â”€ ðŸ“„ QK-OV: steps for computing the QK, OV matrices from the Anthropic paper. Also, bigrams
       and skip-trigrams
```